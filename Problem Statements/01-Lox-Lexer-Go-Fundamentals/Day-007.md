# Day 007: Week 1 Review & Integration Test

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 1 of 4 - Go Setup & Token Types

---

## üéØ Today's Goal

Review Week 1's progress, run comprehensive integration tests, and consolidate your learning. This is your weekly checkpoint to ensure everything is solid before moving to Week 2 (strings, numbers, identifiers).

**What You'll Build:** Integration tests for complex token combinations + week summary document.

---

## üìö What You've Learned This Week

**Go Fundamentals:**
- Module system (`go.mod`)
- Structs and methods
- Enums with `iota`
- Table-driven tests
- Switch statements
- String slicing

**Lexer Components:**
- Token types and Token struct
- Scanner with position tracking
- Single-character token recognition
- Two-character operator lookahead
- Comment handling
- Line number tracking

---

## ‚úÖ Today's Tasks

### Task 1: Write Integration Tests (60 minutes)

**Create file: `pkg/lexer/integration_test.go`**

```go
package lexer

import (
	"testing"

	"github.com/bhargav/konfiguru/pkg/tokens"
)

func TestScanTokens_MixedOperators(t *testing.T) {
	source := "(){} + - * / ! != = == < <= > >="
	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	if len(errs) != 0 {
		t.Fatalf("got errors: %v", errs)
	}

	expected := []tokens.TokenType{
		tokens.LEFT_PAREN, tokens.RIGHT_PAREN,
		tokens.LEFT_BRACE, tokens.RIGHT_BRACE,
		tokens.PLUS, tokens.MINUS, tokens.STAR, tokens.SLASH,
		tokens.BANG, tokens.BANG_EQUAL,
		tokens.EQUAL, tokens.EQUAL_EQUAL,
		tokens.LESS, tokens.LESS_EQUAL,
		tokens.GREATER, tokens.GREATER_EQUAL,
		tokens.EOF,
	}

	if len(toks) != len(expected) {
		t.Fatalf("got %d tokens, want %d", len(toks), len(expected))
	}

	for i, exp := range expected {
		if toks[i].Type != exp {
			t.Errorf("token[%d] = %v, want %v", i, toks[i].Type, exp)
		}
	}
}

func TestScanTokens_WithComments(t *testing.T) {
	source := `
// Line comment
( + ) // another comment
// Final comment
`
	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	if len(errs) != 0 {
		t.Fatalf("got errors: %v", errs)
	}

	// Should only have the tokens between comments
	expected := []tokens.TokenType{
		tokens.LEFT_PAREN,
		tokens.PLUS,
		tokens.RIGHT_PAREN,
		tokens.EOF,
	}

	if len(toks) != len(expected) {
		t.Fatalf("got %d tokens, want %d", len(toks), len(expected))
	}
}

func TestScanTokens_MultiLine(t *testing.T) {
	source := `(
+
-
*
)`
	scanner := NewScanner(source)
	toks, _ := scanner.ScanTokens()

	// Verify line tracking across multiple lines
	expectedLines := []int{1, 2, 3, 4, 5, 5} // EOF on last line
	for i, tok := range toks {
		if tok.Line != expectedLines[i] {
			t.Errorf("token[%d] line = %d, want %d", i, tok.Line, expectedLines[i])
		}
	}
}

func TestScanTokens_ComplexExpression(t *testing.T) {
	source := "!= == >= <= () {}"
	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	if len(errs) != 0 {
		t.Fatalf("got errors: %v", errs)
	}

	// Verify we get exactly the right tokens
	if len(toks) != 9 { // 8 operators + EOF
		t.Fatalf("got %d tokens, want 9", len(toks))
	}
}
```

**Run integration tests:**
```bash
go test ./pkg/lexer -v -run Integration
```

---

### Task 2: Create Week 1 Summary Document (45 minutes)

**Create file: `docs/week-1-summary.md`**

```markdown
# Week 1 Summary: Go Setup & Token Foundation

**Dates:** Day 001 - Day 007
**Status:** ‚úÖ COMPLETE

---

## What We Built

### Complete Token System
- 41 token types defined (operators, keywords, literals, special)
- Token struct with type, lexeme, literal, line tracking
- 15 Lox keywords with lookup map

### Working Scanner
- Scanner struct with position and line tracking
- Single-character token recognition (10 tokens)
- Two-character operators with lookahead (8 combinations)
- Comment support (`//` until newline)
- Whitespace handling
- Error reporting

---

## Statistics

**Code Written:**
- `pkg/tokens/token.go`: ~170 lines
- `pkg/tokens/token_test.go`: ~100 lines
- `pkg/lexer/scanner.go`: ~150 lines
- `pkg/lexer/scanner_test.go`: ~200 lines
- `pkg/lexer/integration_test.go`: ~80 lines

**Total: ~700 lines of production + test code**

**Test Coverage:**
- pkg/tokens: 95%+
- pkg/lexer: 90%+
- Total test cases: 70+

---

## Skills Learned

### Go Programming
‚úÖ Module initialization (`go.mod`)
‚úÖ Package structure (`cmd/`, `pkg/`)
‚úÖ Structs and methods
‚úÖ Enums with `iota`
‚úÖ Maps and slices
‚úÖ Table-driven tests
‚úÖ Test coverage reporting

### Lexer Design
‚úÖ Token classification
‚úÖ Scanner state machine
‚úÖ Position tracking (start, current, line)
‚úÖ Character advancement
‚úÖ Lookahead with peek/match
‚úÖ Error accumulation

---

## What Works

The scanner can now tokenize:
```lox
// Comments are ignored
( ) { } , . - + ; * /
! != = == < <= > >=
```

Example:
```
Input:  "!= == ()"
Output: BANG_EQUAL, EQUAL_EQUAL, LEFT_PAREN, RIGHT_PAREN, EOF
```

---

## Next Week Preview

### Week 2: Literals & Identifiers (Days 8-14)

**Day 8-9:** String literal scanning
- Handle `"quoted strings"`
- Multiline string support
- Unterminated string errors

**Day 10-11:** Number literal scanning
- Integer and floating-point numbers
- Handle decimal points correctly

**Day 12-13:** Identifier and keyword scanning
- Scan variable names
- Distinguish keywords from identifiers

**Day 14:** Week 2 review + complete integration

---

## Achievements Unlocked

‚úÖ First Go project initialized
‚úÖ Complete token type system
‚úÖ Working lexer for operators
‚úÖ Table-driven test suite
‚úÖ 90%+ test coverage
‚úÖ Week 1 complete on schedule!

---

## Reflection

### What Went Well
- Go's simplicity made lexer straightforward
- Table-driven tests caught bugs early
- Small daily commits enabled progress tracking

### Challenges
- Understanding byte vs rune initially
- Getting used to Go's testing style
- Remembering to track line numbers

### Lessons Learned
- TDD saves debugging time later
- Small, focused commits are valuable
- Go's standard library is excellent

---

## Time Tracking

**Week 1 Total:** ~14.5 hours
- Day 1: 2.0 hours (Setup)
- Day 2: 2.2 hours (Token types)
- Day 3: 2.2 hours (Tests)
- Day 4: 2.5 hours (Scanner core)
- Day 5: 2.7 hours (Single-char scanning)
- Day 6: 2.4 hours (Two-char operators)
- Day 7: 0.5 hours (Review)

**On track for 13.5 hrs/week goal!**

---

**Status:** Ready for Week 2 - String/Number/Identifier scanning
```

---

### Task 3: Run Full Test Suite (20 minutes)

**Run all tests with coverage:**
```bash
cd /home/bhargav/Documents/Side-Projects/konfiguru

# Run all tests
go test ./... -v

# Check coverage
go test ./... -cover

# Generate coverage report
go test ./... -coverprofile=coverage.out
go tool cover -func=coverage.out

# View detailed HTML coverage
go tool cover -html=coverage.out -o coverage.html
xdg-open coverage.html
```

**Verify all tests pass:**
- pkg/tokens: All tests ‚úÖ
- pkg/lexer: All tests ‚úÖ

---

### Task 4: Update Main README (20 minutes)

**Update `README.md` in project root:**

Add Week 1 completion status:

```markdown
## Progress Tracker

### Month 1: Lox Lexer (Weeks 1-4)

**Week 1: Token Foundation** ‚úÖ COMPLETE
- [x] Day 1: Go installation & project setup
- [x] Day 2: Token types definition
- [x] Day 3: Token tests
- [x] Day 4: Scanner core structure
- [x] Day 5: Single-character scanning
- [x] Day 6: Two-character operators
- [x] Day 7: Week 1 review

**Achievements:**
- ‚úÖ 41 token types defined
- ‚úÖ Scanner recognizes all operators
- ‚úÖ 70+ test cases, 90%+ coverage
- ‚úÖ Comment support implemented
- ‚úÖ ~700 lines of code

**Next: Week 2 - String/Number/Identifier scanning**
```

---

### Task 5: Commit Week 1 Completion (10 minutes)

```bash
git add .
git commit -m "feat: complete Week 1 - token foundation and operator scanning

Week 1 Summary:
- Complete token type system (41 types)
- Working scanner for all operators
- Single and two-character token support
- Comment handling
- 70+ test cases with 90%+ coverage

Files added:
- pkg/lexer/integration_test.go (comprehensive integration tests)
- docs/week-1-summary.md (week summary)

Ready for Week 2: String, number, and identifier scanning.

Milestone: First week of Lox lexer complete!"
```

---

## ‚úÖ End-of-Day Checklist

- [ ] Integration tests written (4+ test functions)
- [ ] All integration tests passing
- [ ] Week 1 summary document created
- [ ] Full test suite run successfully
- [ ] Coverage report generated and reviewed
- [ ] README updated with Week 1 completion
- [ ] All changes committed to git
- [ ] Week 2 plan reviewed
- [ ] Obsidian notes updated with weekly reflection

**Time Spent:** ~2.5 hours (60+45+20+20+10+review = ~155 minutes)

---

## üîó Navigation

- [‚Üê Day 006: Two-Character Operators](Day-006.md)
- [‚Üí Day 008: String Literal Scanning](Day-008.md)
- [‚Üë Back to Month 1](README.md)

---

## üìù Learning Notes

**What did we accomplish this week?**
You built a working lexer that scans all Lox operators and handles comments. This is the foundation of any compiler!

**Week 1 vs Week 2:**
- Week 1: Operators (fixed tokens)
- Week 2: Literals (variable-length tokens)

Operators are easy - single or double character. Literals like strings and numbers require scanning until a delimiter.

**Key insight:**
The lexer is a **state machine**. Each character moves the scanner forward, updating state (position, line, current token). This pattern scales to much more complex languages.

**Next week preview:**
String scanning: `"hello world"` ‚Üí STRING token with literal "hello world"
Number scanning: `123.45` ‚Üí NUMBER token with literal 123.45
Identifier scanning: `var` vs `variable` ‚Üí VAR vs IDENTIFIER

---

*Progress: Day 7/30 complete (Week 1 done!)* ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**WEEK 1 COMPLETE!** You've built the foundation of a lexer. Next week: literals!
