# Day 017: Error Testing & Edge Cases

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 3 of 4 - Integration Testing & REPL Development

---

## üéØ Today's Goal

Thoroughly test error handling and edge cases. Ensure the lexer gracefully handles invalid input, boundary conditions, and recovers from errors to continue scanning.

**What You'll Build:** Comprehensive error and edge case tests.

---

## üìö What You'll Learn Today

**Go Fundamentals:**
- Error testing patterns
- Boundary condition testing
- Table-driven tests for error cases

**Compiler Concepts:**
- Error recovery strategies
- Graceful degradation
- Lexer robustness principles

---

## ‚úÖ Today's Tasks

### Task 1: Test Unterminated String Errors (30 minutes)

**Add to `pkg/lexer/scanner_test.go` (or create error_test.go):**

```go
func TestScanTokens_UnterminatedStrings(t *testing.T) {
	tests := []struct {
		name   string
		source string
	}{
		{"simple unterminated", `"hello`},
		{"unterminated at EOF", `var x = "test`},
		{"multiline unterminated", `"hello
world
no closing quote`},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			_, errs := scanner.ScanTokens()

			if len(errs) == 0 {
				t.Error("expected error for unterminated string")
			}

			// Verify error message mentions "Unterminated"
			found := false
			for _, err := range errs {
				if strings.Contains(err, "Unterminated") {
					found = true
					break
				}
			}
			if !found {
				t.Errorf("error should mention 'Unterminated', got: %v", errs)
			}
		})
	}
}
```

**Run:**
```bash
go test ./pkg/lexer -v -run TestScanTokens_Unterminated
```

---

### Task 2: Test Invalid Character Errors (30 minutes)

**Add to error tests:**

```go
func TestScanTokens_InvalidCharacters(t *testing.T) {
	tests := []struct {
		name           string
		source         string
		invalidChar    rune
	}{
		{"at symbol", "var x @ 5;", '@'},
		{"hash symbol", "var y # 10;", '#'},
		{"backtick", "var z ` test;", '`'},
		{"dollar sign", "var a $ 100;", '$'},
		{"percent (standalone)", "var b % 50;", '%'},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			_, errs := scanner.ScanTokens()

			if len(errs) == 0 {
				t.Errorf("expected error for invalid character %c", tt.invalidChar)
			}

			// Verify error message mentions "Unexpected character"
			found := false
			for _, err := range errs {
				if strings.Contains(err, "Unexpected character") {
					found = true
					break
				}
			}
			if !found {
				t.Errorf("error should mention 'Unexpected character', got: %v", errs)
			}
		})
	}
}
```

---

### Task 3: Test Boundary Conditions (45 minutes)

**Add boundary tests:**

```go
func TestScanTokens_BoundaryConditions(t *testing.T) {
	tests := []struct {
		name   string
		source string
		verify func(t *testing.T, toks []tokens.Token, errs []string)
	}{
		{
			name:   "empty string",
			source: "",
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("empty string should not error, got: %v", errs)
				}
				if len(toks) != 1 || toks[0].Type != tokens.EOF {
					t.Error("empty string should only produce EOF token")
				}
			},
		},
		{
			name:   "only whitespace",
			source: "   \t\n\r\n   ",
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("whitespace should not error, got: %v", errs)
				}
				if len(toks) != 1 || toks[0].Type != tokens.EOF {
					t.Error("whitespace should only produce EOF token")
				}
			},
		},
		{
			name:   "only comments",
			source: "// comment 1\n// comment 2\n// comment 3",
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("comments should not error, got: %v", errs)
				}
				if len(toks) != 1 || toks[0].Type != tokens.EOF {
					t.Error("comments should only produce EOF token")
				}
			},
		},
		{
			name:   "very long identifier",
			source: strings.Repeat("a", 1000),
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("long identifier should not error, got: %v", errs)
				}
				if len(toks) != 2 {
					t.Fatalf("expected 2 tokens (IDENTIFIER + EOF), got %d", len(toks))
				}
				if toks[0].Type != tokens.IDENTIFIER {
					t.Error("first token should be IDENTIFIER")
				}
			},
		},
		{
			name:   "very long number",
			source: "123456789012345678901234567890.987654321098765432109876543210",
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("long number should not error, got: %v", errs)
				}
				if len(toks) != 2 {
					t.Fatalf("expected 2 tokens (NUMBER + EOF), got %d", len(toks))
				}
				if toks[0].Type != tokens.NUMBER {
					t.Error("first token should be NUMBER")
				}
			},
		},
		{
			name:   "number starting with zero",
			source: "0 00 000",
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("zeros should not error, got: %v", errs)
				}
				// Should produce 3 NUMBER tokens + EOF
				numberCount := 0
				for _, tok := range toks {
					if tok.Type == tokens.NUMBER {
						numberCount++
					}
				}
				if numberCount != 3 {
					t.Errorf("expected 3 NUMBER tokens, got %d", numberCount)
				}
			},
		},
		{
			name:   "empty string literal",
			source: `""`,
			verify: func(t *testing.T, toks []tokens.Token, errs []string) {
				if len(errs) != 0 {
					t.Errorf("empty string literal should not error, got: %v", errs)
				}
				if len(toks) != 2 {
					t.Fatalf("expected 2 tokens (STRING + EOF), got %d", len(toks))
				}
				if toks[0].Type != tokens.STRING {
					t.Error("should produce STRING token")
				}
				if toks[0].Literal != "" {
					t.Errorf("empty string literal value should be empty, got %q", toks[0].Literal)
				}
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			toks, errs := scanner.ScanTokens()
			tt.verify(t, toks, errs)
		})
	}
}
```

**Run:**
```bash
go test ./pkg/lexer -v -run TestScanTokens_Boundary
```

---

### Task 4: Test Edge Cases with Operators (30 minutes)

**Add operator edge case tests:**

```go
func TestScanTokens_OperatorEdgeCases(t *testing.T) {
	tests := []struct {
		name     string
		source   string
		expected []tokens.TokenType
	}{
		{
			name:   "consecutive operators",
			source: "+-*/",
			expected: []tokens.TokenType{
				tokens.PLUS, tokens.MINUS, tokens.STAR, tokens.SLASH, tokens.EOF,
			},
		},
		{
			name:   "ambiguous operators",
			source: "! != = == < <= > >=",
			expected: []tokens.TokenType{
				tokens.BANG, tokens.BANG_EQUAL,
				tokens.EQUAL, tokens.EQUAL_EQUAL,
				tokens.LESS, tokens.LESS_EQUAL,
				tokens.GREATER, tokens.GREATER_EQUAL,
				tokens.EOF,
			},
		},
		{
			name:   "slash vs comment",
			source: "/ // /",
			expected: []tokens.TokenType{
				tokens.SLASH, tokens.EOF,
			},
		},
		{
			name:   "dot with numbers",
			source: "1.5 . 2.3",
			expected: []tokens.TokenType{
				tokens.NUMBER, tokens.DOT, tokens.NUMBER, tokens.EOF,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			toks, errs := scanner.ScanTokens()

			if len(errs) != 0 {
				t.Fatalf("unexpected errors: %v", errs)
			}

			if len(toks) != len(tt.expected) {
				t.Fatalf("got %d tokens, want %d", len(toks), len(tt.expected))
			}

			for i, exp := range tt.expected {
				if toks[i].Type != exp {
					t.Errorf("token[%d] = %v, want %v", i, toks[i].Type, exp)
				}
			}
		})
	}
}
```

---

### Task 5: Test Error Recovery (45 minutes)

**Test that scanner continues after errors:**

```go
func TestScanTokens_ErrorRecovery(t *testing.T) {
	// Source with multiple errors
	source := `var x = 5;
var y @ 10;
var z = "unterminated
var a # invalid;
var b = 15;`

	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	// Should have errors
	if len(errs) == 0 {
		t.Fatal("expected errors for invalid syntax")
	}

	// But should still produce valid tokens
	// Should have at least VAR, IDENTIFIER, EQUAL, NUMBER tokens
	hasVar := false
	hasNumber := false
	for _, tok := range toks {
		if tok.Type == tokens.VAR {
			hasVar = true
		}
		if tok.Type == tokens.NUMBER {
			hasNumber = true
		}
	}

	if !hasVar {
		t.Error("scanner should recover and scan VAR keywords")
	}
	if !hasNumber {
		t.Error("scanner should recover and scan valid numbers")
	}

	// Verify we got multiple errors
	if len(errs) < 2 {
		t.Errorf("expected multiple errors, got %d", len(errs))
	}
}

func TestScanTokens_ErrorsHaveLineNumbers(t *testing.T) {
	source := `var x = 5;
var y @ 10;
var z # 15;`

	scanner := NewScanner(source)
	_, errs := scanner.ScanTokens()

	if len(errs) < 2 {
		t.Fatal("expected at least 2 errors")
	}

	// Each error should mention a line number
	for _, err := range errs {
		if !strings.Contains(err, "line") {
			t.Errorf("error should mention line number: %s", err)
		}
	}
}
```

---

### Task 6: Run All Error Tests (20 minutes)

**Run complete error test suite:**
```bash
# Run all error-related tests
go test ./pkg/lexer -v -run ".*Error|.*Boundary|.*Edge"

# Run all tests
go test ./pkg/lexer -v

# Check coverage
go test ./pkg/lexer -cover
```

**Verify:**
- All tests passing
- Error messages are helpful
- Scanner recovers from errors
- Coverage remains high (>85%)

---

### Task 7: Commit Error Tests (10 minutes)

**Git commit:**
```bash
git add pkg/lexer/
git commit -m "test: add comprehensive error and edge case tests

- Test unterminated string error handling
- Test invalid character detection
- Test boundary conditions (empty input, whitespace, long tokens)
- Test operator edge cases (consecutive operators, ambiguous sequences)
- Test error recovery (scanner continues after errors)
- Verify error messages include line numbers
- Ensure lexer handles all edge cases gracefully

Day 17/30: Error handling and edge cases complete"
```

---

## üìñ Resources

**Error Handling:**
- [Effective Error Handling in Go](https://go.dev/blog/error-handling-and-go)
- [Crafting Interpreters - Error Reporting](https://craftinginterpreters.com/scanning.html#error-handling)

---

## ‚úÖ End-of-Day Checklist

- [ ] Wrote tests for unterminated strings
- [ ] Wrote tests for invalid characters
- [ ] Wrote boundary condition tests
- [ ] Wrote operator edge case tests
- [ ] Wrote error recovery tests
- [ ] All tests passing: `go test ./pkg/lexer -v`
- [ ] Coverage >85%
- [ ] Code committed to git
- [ ] Updated Obsidian notes

**Time Spent:** ~1 hour (30+30+45+30+45+20+10 = 210 minutes = 3.5 hours)

---

## üîó Navigation

- [‚Üê Day 016: Integration Tests - Complex Programs](Day-016.md)
- [‚Üí Day 018: CLI REPL (Part 1)](Day-018.md)
- [‚Üë Month Overview](README.md)

---

## üìù Learning Notes

**Error Testing Principles:**

1. **Test Invalid Input:**
   - Unterminated strings
   - Invalid characters
   - Malformed numbers

2. **Test Boundary Conditions:**
   - Empty input
   - Very long tokens
   - Edge values (0, empty strings)

3. **Test Error Recovery:**
   - Scanner continues after errors
   - Multiple errors reported
   - Valid tokens still produced

4. **Test Error Messages:**
   - Include line numbers
   - Clear, descriptive messages
   - Help users fix their code

**What Makes a Good Error Message:**
- **Location:** Line number where error occurred
- **Context:** What the lexer was doing
- **Problem:** What went wrong
- **Example:** "[line 3] Error: Unterminated string."

**Robustness Principles:**
- Don't crash on invalid input
- Report multiple errors, not just the first
- Continue scanning after errors
- Produce useful error messages

**Tomorrow Preview:**
CLI REPL development - building the interactive command-line interface.

---

*Progress: Day 17/30 complete* ‚≠ê
*Week 3: Integration Testing & REPL Development*

**Great work! Your lexer is now robust and production-ready!**
