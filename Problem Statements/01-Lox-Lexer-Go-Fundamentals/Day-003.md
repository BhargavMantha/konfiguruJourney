# Day 003: Token Types Tests & Verification

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 1 of 4 - Go Setup & Token Types

---

## ğŸ¯ Today's Goal

Write comprehensive table-driven tests for your token types to ensure they work correctly before building the scanner. Testing first is a TDD best practice that catches bugs early and documents expected behavior.

**What You'll Build:** Complete test suite for token types with 20+ test cases.

---

## ğŸ“š What You'll Learn Today

**Go Testing:**
- Table-driven test pattern (Go best practice)
- Writing test functions with `*testing.T`
- Test assertions and error reporting
- Running and organizing tests

**TDD Philosophy:**
- Write tests BEFORE implementation
- Tests as documentation
- Red-Green-Refactor cycle

---

## âœ… Today's Tasks

### Task 1: Write Token String Tests (30 minutes)

**What to do:**
Create comprehensive tests for the Token.String() method using table-driven testing.

**Create file: `pkg/tokens/token_test.go`**

```go
package tokens

import "testing"

func TestTokenString(t *testing.T) {
	tests := []struct {
		name      string
		tokenType TokenType
		lexeme    string
		literal   interface{}
		line      int
		want      string
	}{
		{
			name:      "left paren token",
			tokenType: LEFT_PAREN,
			lexeme:    "(",
			literal:   nil,
			line:      1,
			want:      "LEFT_PAREN ( <nil>",
		},
		{
			name:      "number token with literal",
			tokenType: NUMBER,
			lexeme:    "123",
			literal:   123.0,
			line:      1,
			want:      "NUMBER 123 123",
		},
		{
			name:      "string token with literal",
			tokenType: STRING,
			lexeme:    "\"hello\"",
			literal:   "hello",
			line:      1,
			want:      "STRING \"hello\" hello",
		},
		{
			name:      "identifier token",
			tokenType: IDENTIFIER,
			lexeme:    "foo",
			literal:   nil,
			line:      1,
			want:      "IDENTIFIER foo <nil>",
		},
		{
			name:      "var keyword",
			tokenType: VAR,
			lexeme:    "var",
			literal:   nil,
			line:      5,
			want:      "VAR var <nil>",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			token := NewToken(tt.tokenType, tt.lexeme, tt.literal, tt.line)
			got := token.String()
			if got != tt.want {
				t.Errorf("Token.String() = %q, want %q", got, tt.want)
			}
		})
	}
}
```

**Why table-driven tests?**
- Test multiple cases with same logic
- Easy to add new cases
- Self-documenting (each case has a name)
- Go community best practice

**Run the tests:**
```bash
cd /home/bhargav/Documents/Side-Projects/konfiguru
go test ./pkg/tokens -v
```

**Expected output:**
```
=== RUN   TestTokenString
=== RUN   TestTokenString/left_paren_token
=== RUN   TestTokenString/number_token_with_literal
=== RUN   TestTokenString/string_token_with_literal
=== RUN   TestTokenString/identifier_token
=== RUN   TestTokenString/var_keyword
--- PASS: TestTokenString (0.00s)
    --- PASS: TestTokenString/left_paren_token (0.00s)
    --- PASS: TestTokenString/number_token_with_literal (0.00s)
    --- PASS: TestTokenString/string_token_with_literal (0.00s)
    --- PASS: TestTokenString/identifier_token (0.00s)
    --- PASS: TestTokenString/var_keyword (0.00s)
PASS
ok      github.com/bhargav/konfiguru/pkg/tokens
```

---

### Task 2: Write Keyword Lookup Tests (30 minutes)

**What to do:**
Test the LookupIdent function to ensure keywords are correctly distinguished from identifiers.

**Add to `pkg/tokens/token_test.go`:**

```go
func TestLookupIdent(t *testing.T) {
	tests := []struct {
		name  string
		ident string
		want  TokenType
	}{
		// All keywords
		{"and keyword", "and", AND},
		{"class keyword", "class", CLASS},
		{"else keyword", "else", ELSE},
		{"false keyword", "false", FALSE},
		{"for keyword", "for", FOR},
		{"fun keyword", "fun", FUN},
		{"if keyword", "if", IF},
		{"nil keyword", "nil", NIL},
		{"or keyword", "or", OR},
		{"print keyword", "print", PRINT},
		{"return keyword", "return", RETURN},
		{"super keyword", "super", SUPER},
		{"this keyword", "this", THIS},
		{"true keyword", "true", TRUE},
		{"var keyword", "var", VAR},
		{"while keyword", "while", WHILE},

		// Identifiers that look like keywords
		{"variable identifier", "variable", IDENTIFIER},
		{"foo identifier", "foo", IDENTIFIER},
		{"bar123 identifier", "bar123", IDENTIFIER},
		{"_underscore identifier", "_underscore", IDENTIFIER},
		{"CamelCase identifier", "CamelCase", IDENTIFIER},

		// Edge cases
		{"VAR uppercase", "VAR", IDENTIFIER}, // Keywords are case-sensitive
		{"Var mixed case", "Var", IDENTIFIER},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := LookupIdent(tt.ident)
			if got != tt.want {
				t.Errorf("LookupIdent(%q) = %v, want %v", tt.ident, got, tt.want)
			}
		})
	}
}
```

**Key insights from these tests:**
1. All 15 Lox keywords are tested
2. Similar-looking identifiers are tested (e.g., "var" vs "variable")
3. Case sensitivity is verified (Lox keywords are lowercase only)

**Run tests:**
```bash
go test ./pkg/tokens -v -run TestLookupIdent
```

---

### Task 3: Write TokenType String Tests (20 minutes)

**What to do:**
Test that TokenType.String() returns the correct string for each token type.

**Add to `pkg/tokens/token_test.go`:**

```go
func TestTokenTypeString(t *testing.T) {
	tests := []struct {
		name string
		typ  TokenType
		want string
	}{
		// Single-character tokens
		{"left paren", LEFT_PAREN, "LEFT_PAREN"},
		{"right paren", RIGHT_PAREN, "RIGHT_PAREN"},
		{"left brace", LEFT_BRACE, "LEFT_BRACE"},
		{"semicolon", SEMICOLON, "SEMICOLON"},
		{"plus", PLUS, "PLUS"},
		{"star", STAR, "STAR"},

		// Two-character tokens
		{"bang equal", BANG_EQUAL, "BANG_EQUAL"},
		{"equal equal", EQUAL_EQUAL, "EQUAL_EQUAL"},
		{"less equal", LESS_EQUAL, "LESS_EQUAL"},
		{"greater equal", GREATER_EQUAL, "GREATER_EQUAL"},

		// Literals
		{"identifier", IDENTIFIER, "IDENTIFIER"},
		{"string", STRING, "STRING"},
		{"number", NUMBER, "NUMBER"},

		// Keywords
		{"var", VAR, "VAR"},
		{"while", WHILE, "WHILE"},
		{"fun", FUN, "FUN"},
		{"class", CLASS, "CLASS"},

		// Special
		{"eof", EOF, "EOF"},
		{"illegal", ILLEGAL, "ILLEGAL"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tt.typ.String()
			if got != tt.want {
				t.Errorf("TokenType.String() = %q, want %q", got, tt.want)
			}
		})
	}
}
```

**Run all tests:**
```bash
go test ./pkg/tokens -v
```

---

### Task 4: Add NewToken Constructor Test (15 minutes)

**What to do:**
Test that NewToken correctly constructs tokens with all fields.

**Add to `pkg/tokens/token_test.go`:**

```go
func TestNewToken(t *testing.T) {
	tests := []struct {
		name      string
		tokenType TokenType
		lexeme    string
		literal   interface{}
		line      int
	}{
		{
			name:      "simple operator",
			tokenType: PLUS,
			lexeme:    "+",
			literal:   nil,
			line:      1,
		},
		{
			name:      "number with literal",
			tokenType: NUMBER,
			lexeme:    "42.5",
			literal:   42.5,
			line:      10,
		},
		{
			name:      "string with literal",
			tokenType: STRING,
			lexeme:    "\"test\"",
			literal:   "test",
			line:      7,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			token := NewToken(tt.tokenType, tt.lexeme, tt.literal, tt.line)

			if token.Type != tt.tokenType {
				t.Errorf("token.Type = %v, want %v", token.Type, tt.tokenType)
			}
			if token.Lexeme != tt.lexeme {
				t.Errorf("token.Lexeme = %q, want %q", token.Lexeme, tt.lexeme)
			}
			if token.Literal != tt.literal {
				t.Errorf("token.Literal = %v, want %v", token.Literal, tt.literal)
			}
			if token.Line != tt.line {
				t.Errorf("token.Line = %d, want %d", token.Line, tt.line)
			}
		})
	}
}
```

---

### Task 5: Run Full Test Suite and Verify Coverage (20 minutes)

**What to do:**
Run all tests with coverage reporting to ensure comprehensive testing.

**Run with coverage:**
```bash
go test ./pkg/tokens -v -cover
```

**Expected output:**
```
=== RUN   TestTokenString
--- PASS: TestTokenString (0.00s)
=== RUN   TestLookupIdent
--- PASS: TestLookupIdent (0.00s)
=== RUN   TestTokenTypeString
--- PASS: TestTokenTypeString (0.00s)
=== RUN   TestNewToken
--- PASS: TestNewToken (0.00s)
PASS
coverage: 95.2% of statements
ok      github.com/bhargav/konfiguru/pkg/tokens
```

**Generate detailed coverage report:**
```bash
go test ./pkg/tokens -coverprofile=coverage.out
go tool cover -html=coverage.out -o coverage.html
```

This creates an HTML file showing which lines are covered by tests.

**View coverage:**
```bash
xdg-open coverage.html  # On Linux
# or just open in browser: firefox coverage.html
```

**Clean up coverage files (don't commit them):**
```bash
rm coverage.out coverage.html
```

---

### Task 6: Commit Test Suite (15 minutes)

**What to do:**
Commit your comprehensive test suite.

```bash
git status
# Should show: pkg/tokens/token_test.go (new file)

git add pkg/tokens/token_test.go
git commit -m "test: add comprehensive token types test suite

- Add table-driven tests for Token.String()
- Test all 15 keywords with LookupIdent()
- Test TokenType.String() for all token types
- Test NewToken() constructor
- Achieve 95%+ test coverage

Tests verify:
- Token construction and string representation
- Keyword vs identifier distinction
- Case sensitivity of keywords
- All token type enum values

This establishes test foundation before scanner implementation."
```

**Verify commit:**
```bash
git log --oneline -n 2
```

---

## ğŸ“– Resources

**Essential:**
- [Go Testing Package](https://pkg.go.dev/testing)
- [Go by Example: Testing](https://gobyexample.com/testing)
- [Table Driven Tests in Go](https://dave.cheney.net/2019/05/07/prefer-table-driven-tests)

**Advanced:**
- [Go Code Coverage](https://go.dev/blog/cover)
- [Testify Package](https://github.com/stretchr/testify) (optional assertion library)

---

## âœ… End-of-Day Checklist

- [ ] `pkg/tokens/token_test.go` created
- [ ] TestTokenString written with 5+ cases
- [ ] TestLookupIdent written with all 15 keywords + identifiers
- [ ] TestTokenTypeString written for major token types
- [ ] TestNewToken written for constructor verification
- [ ] All tests passing: `go test ./pkg/tokens`
- [ ] Coverage > 90%: `go test ./pkg/tokens -cover`
- [ ] Coverage report generated and reviewed
- [ ] Tests committed to git
- [ ] Tomorrow's plan reviewed (Day 004: Scanner core structure)

**Time Spent:** ~2.2 hours (30+30+20+15+20+15 = 130 minutes)

**What you built today:**
```
token_test.go
â”œâ”€â”€ TestTokenString         (5 test cases)
â”œâ”€â”€ TestLookupIdent         (23 test cases)
â”œâ”€â”€ TestTokenTypeString     (20 test cases)
â””â”€â”€ TestNewToken            (3 test cases)

Total: 51+ test cases, 95%+ coverage
```

---

## ğŸ”— Navigation

- [â† Day 002: Token Types Definition](Day-002.md)
- [â†’ Day 004: Scanner Core Structure](Day-004.md)
- [â†‘ Back to Month 1](README.md)

---

## ğŸ“ Learning Notes

**What did we accomplish?**
You wrote comprehensive tests BEFORE implementing the scanner. This is true Test-Driven Development. Now when you write the scanner, you'll know immediately if token generation is correct.

**Key Go testing patterns learned:**
1. **Table-driven tests**: The `tests := []struct{...}` pattern
2. **t.Run()**: Creates subtests for better organization
3. **Test naming**: Descriptive names explain intent
4. **-v flag**: Verbose output shows each subtest
5. **-cover flag**: Measures test coverage percentage

**Why table-driven tests are powerful:**
```go
// Instead of writing:
func TestCase1() { ... }
func TestCase2() { ... }
func TestCase3() { ... }

// Write:
tests := []struct{...}{case1, case2, case3}
for _, tt := range tests {
    t.Run(tt.name, func(t *testing.T) {
        // Single test logic
    })
}
```

Benefits:
- Less code duplication
- Easy to add cases
- Clear test data structure
- Better failure messages

**Coverage insights:**
- 95%+ coverage is excellent
- Uncovered lines are often error paths (that's okay)
- Use coverage to find gaps, not as a metric to game

**Tomorrow's preview:**
You'll build the Scanner struct - the core of your lexer that reads source code character by character and produces tokens.

---

*Progress: Day 3/30 complete* â­â­â­
*Module: Token Types Testing*

**TDD in action:** Tests written, all passing. Scanner implementation next!
