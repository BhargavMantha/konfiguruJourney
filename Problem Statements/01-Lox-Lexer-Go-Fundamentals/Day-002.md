# Day 002: Token Types Definition (Part 1)

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 1 of 4 - Go Setup & Token Types

---

## üéØ Today's Goal

Define all token types that the Lox lexer will recognize and create the foundational Token struct. This is where you establish the vocabulary of the Lox language - every keyword, operator, and literal type that your compiler will understand.

**What You'll Build:** Complete token type system with enums, structs, and keyword mapping.

---

## üìö What You'll Learn Today

**Go Fundamentals:**
- Enums using `const` with `iota` (Go's enum pattern)
- Struct definitions and constructors
- Method receivers and the `String()` method
- Map initialization and lookups
- Package organization

**Compiler Concepts:**
- Token classification (operators, literals, keywords)
- Lexeme vs literal values
- Keyword reservation and lookup

---

## ‚úÖ Today's Tasks

### Task 1: Define TokenType Enum (40 minutes)

**What to do:**
Create the `TokenType` enum that represents every possible token in Lox.

**Step-by-step:**

1. **Create the token types file:**
```bash
cd /home/bhargav/Documents/Side-Projects/konfiguru
touch pkg/tokens/token.go
```

2. **Define the TokenType enum:**

Create file: `pkg/tokens/token.go`

```go
package tokens

import "fmt"

// TokenType represents the type of a token
type TokenType int

const (
	// Single-character tokens
	LEFT_PAREN TokenType = iota
	RIGHT_PAREN
	LEFT_BRACE
	RIGHT_BRACE
	COMMA
	DOT
	MINUS
	PLUS
	SEMICOLON
	SLASH
	STAR

	// One or two character tokens
	BANG
	BANG_EQUAL
	EQUAL
	EQUAL_EQUAL
	GREATER
	GREATER_EQUAL
	LESS
	LESS_EQUAL

	// Literals
	IDENTIFIER
	STRING
	NUMBER

	// Keywords
	AND
	CLASS
	ELSE
	FALSE
	FUN
	FOR
	IF
	NIL
	OR
	PRINT
	RETURN
	SUPER
	THIS
	TRUE
	VAR
	WHILE

	// Special
	EOF
	ILLEGAL
)
```

**Why this structure?**
- `iota` automatically assigns incrementing values (0, 1, 2, ...)
- Grouped by category for readability
- `EOF` marks end of input
- `ILLEGAL` for unrecognized characters

3. **Add String() method for debugging:**

Add to `pkg/tokens/token.go`:

```go
// String returns the string representation of a TokenType
func (t TokenType) String() string {
	switch t {
	case LEFT_PAREN:
		return "LEFT_PAREN"
	case RIGHT_PAREN:
		return "RIGHT_PAREN"
	case LEFT_BRACE:
		return "LEFT_BRACE"
	case RIGHT_BRACE:
		return "RIGHT_BRACE"
	case COMMA:
		return "COMMA"
	case DOT:
		return "DOT"
	case MINUS:
		return "MINUS"
	case PLUS:
		return "PLUS"
	case SEMICOLON:
		return "SEMICOLON"
	case SLASH:
		return "SLASH"
	case STAR:
		return "STAR"
	case BANG:
		return "BANG"
	case BANG_EQUAL:
		return "BANG_EQUAL"
	case EQUAL:
		return "EQUAL"
	case EQUAL_EQUAL:
		return "EQUAL_EQUAL"
	case GREATER:
		return "GREATER"
	case GREATER_EQUAL:
		return "GREATER_EQUAL"
	case LESS:
		return "LESS"
	case LESS_EQUAL:
		return "LESS_EQUAL"
	case IDENTIFIER:
		return "IDENTIFIER"
	case STRING:
		return "STRING"
	case NUMBER:
		return "NUMBER"
	case AND:
		return "AND"
	case CLASS:
		return "CLASS"
	case ELSE:
		return "ELSE"
	case FALSE:
		return "FALSE"
	case FUN:
		return "FUN"
	case FOR:
		return "FOR"
	case IF:
		return "IF"
	case NIL:
		return "NIL"
	case OR:
		return "OR"
	case PRINT:
		return "PRINT"
	case RETURN:
		return "RETURN"
	case SUPER:
		return "SUPER"
	case THIS:
		return "THIS"
	case TRUE:
		return "TRUE"
	case VAR:
		return "VAR"
	case WHILE:
		return "WHILE"
	case EOF:
		return "EOF"
	case ILLEGAL:
		return "ILLEGAL"
	default:
		return "UNKNOWN"
	}
}
```

**Verification:**
```bash
go build ./pkg/tokens
# Should complete without errors
```

---

### Task 2: Define Token Struct (30 minutes)

**What to do:**
Create the Token struct that holds all information about a scanned token.

**Add to `pkg/tokens/token.go`:**

```go
// Token represents a single token in the Lox language
type Token struct {
	Type    TokenType   // The type of the token
	Lexeme  string      // The raw text of the token
	Literal interface{} // The literal value (for numbers, strings)
	Line    int         // Line number where the token appears
}

// NewToken creates a new token
func NewToken(tokenType TokenType, lexeme string, literal interface{}, line int) Token {
	return Token{
		Type:    tokenType,
		Lexeme:  lexeme,
		Literal: literal,
		Line:    line,
	}
}

// String returns a string representation of the token
func (t Token) String() string {
	return fmt.Sprintf("%s %s %v", t.Type, t.Lexeme, t.Literal)
}
```

**Understanding the fields:**
- `Type`: What kind of token (VAR, PLUS, NUMBER, etc.)
- `Lexeme`: The actual text from source code ("var", "+", "123")
- `Literal`: Parsed value for numbers/strings (123.0, "hello")
- `Line`: For error messages ("Error on line 5...")

**Example tokens:**
```
Token{Type: VAR, Lexeme: "var", Literal: nil, Line: 1}
Token{Type: NUMBER, Lexeme: "123", Literal: 123.0, Line: 1}
Token{Type: STRING, Lexeme: "\"hello\"", Literal: "hello", Line: 1}
```

---

### Task 3: Add Keyword Map (30 minutes)

**What to do:**
Create a map to distinguish keywords from identifiers.

**Add to `pkg/tokens/token.go`:**

```go
// Keywords maps Lox keywords to their token types
var Keywords = map[string]TokenType{
	"and":    AND,
	"class":  CLASS,
	"else":   ELSE,
	"false":  FALSE,
	"for":    FOR,
	"fun":    FUN,
	"if":     IF,
	"nil":    NIL,
	"or":     OR,
	"print":  PRINT,
	"return": RETURN,
	"super":  SUPER,
	"this":   THIS,
	"true":   TRUE,
	"var":    VAR,
	"while":  WHILE,
}

// LookupIdent checks if an identifier is a keyword
func LookupIdent(ident string) TokenType {
	if tok, ok := Keywords[ident]; ok {
		return tok
	}
	return IDENTIFIER
}
```

**Why this matters:**
When the lexer sees "var", it needs to know it's a keyword (VAR token), not a regular identifier. When it sees "variable", that's just an IDENTIFIER.

**How it works:**
```go
LookupIdent("var")      // Returns: VAR
LookupIdent("variable") // Returns: IDENTIFIER
LookupIdent("while")    // Returns: WHILE
LookupIdent("foo")      // Returns: IDENTIFIER
```

**Verification:**
```bash
go build ./pkg/tokens
# Should compile successfully
```

---

### Task 4: Verify and Test Manually (20 minutes)

**What to do:**
Create a simple test program to verify your token types work.

**Create file: `pkg/tokens/manual_test.go`** (temporary, for learning)

```go
package tokens

import "fmt"

func ExampleTokenTypes() {
	// Create some example tokens
	tokens := []Token{
		NewToken(VAR, "var", nil, 1),
		NewToken(IDENTIFIER, "x", nil, 1),
		NewToken(EQUAL, "=", nil, 1),
		NewToken(NUMBER, "5", 5.0, 1),
		NewToken(SEMICOLON, ";", nil, 1),
	}

	for _, tok := range tokens {
		fmt.Println(tok)
	}
}

func ExampleKeywordLookup() {
	// Test keyword lookup
	words := []string{"var", "variable", "while", "foo"}

	for _, word := range words {
		tokenType := LookupIdent(word)
		fmt.Printf("%s -> %s\n", word, tokenType)
	}
}
```

**Run manually:**
```bash
cd pkg/tokens
go run manual_test.go token.go
```

**Expected output:**
```
VAR var <nil>
IDENTIFIER x <nil>
EQUAL = <nil>
NUMBER 5 5
SEMICOLON ; <nil>

var -> VAR
variable -> IDENTIFIER
while -> WHILE
foo -> IDENTIFIER
```

**After verification, remove the manual test:**
```bash
rm pkg/tokens/manual_test.go
```

---

### Task 5: Commit Your Work (10 minutes)

**What to do:**
Commit the token types to git.

```bash
git status
# Should show: pkg/tokens/token.go

git add pkg/tokens/token.go
git commit -m "feat: define Lox token types and Token struct

- Add TokenType enum with all Lox tokens (single-char, operators, keywords, literals)
- Implement Token struct with type, lexeme, literal, line tracking
- Add Keywords map for keyword lookup
- Implement String() methods for debugging
- Add LookupIdent() to distinguish keywords from identifiers

Ref: Crafting Interpreters Chapter 4 (Scanning)"
```

---

## üìñ Resources

**Essential:**
- [Crafting Interpreters - 4.2 Lexemes and Tokens](https://craftinginterpreters.com/scanning.html#lexemes-and-tokens)
- [Go by Example: Enums](https://gobyexample.com/constants)
- [Go by Example: Structs](https://gobyexample.com/structs)

**Go Deep Dive:**
- [Effective Go - Constants](https://go.dev/doc/effective_go#constants)
- [Go Maps in Action](https://go.dev/blog/maps)

---

## ‚úÖ End-of-Day Checklist

- [ ] `pkg/tokens/token.go` created
- [ ] TokenType enum defined with all 41 token types
- [ ] Token struct defined with 4 fields
- [ ] NewToken() constructor implemented
- [ ] String() methods for both TokenType and Token
- [ ] Keywords map with all 15 Lox keywords
- [ ] LookupIdent() function working
- [ ] Code compiles: `go build ./pkg/tokens`
- [ ] Manual verification completed
- [ ] Committed to git
- [ ] Tomorrow's plan reviewed (Day 003: Writing proper tests)

**Time Spent:** ~2.2 hours (40+30+30+20+10 = 130 minutes)

**What you built today:**
```go
// The foundation of your lexer
type TokenType int   // 41 token types defined
type Token struct    // Holds token data
Keywords map         // 15 keywords
LookupIdent()        // Keyword vs identifier
```

---

## üîó Navigation

- [‚Üê Day 001: Project Setup](Day-001.md)
- [‚Üí Day 003: Token Types Tests](Day-003.md)
- [‚Üë Back to Month 1](README.md)

---

## üìù Learning Notes

**What did we accomplish?**
Today you defined the complete vocabulary of the Lox language. Every token the lexer will ever produce is defined in this file.

**Key Go concepts learned:**
1. **iota**: Go's way of creating enums (auto-incrementing constants)
2. **Method receivers**: `(t TokenType)` makes String() a method on TokenType
3. **interface{}**: Go's "any type" - needed for Literal field (can be string, float64, or nil)
4. **Map literals**: `map[string]TokenType{...}` creates a lookup table

**Why Keywords map matters:**
Without it, "var" would be scanned as an identifier. The map lets us check: "Is this word special?" If yes, return the keyword token; if no, it's just an identifier.

**Common gotcha:**
In Go, `interface{}` means "any type". The Literal field can be:
- `nil` for keywords and operators
- `float64` for numbers
- `string` for string literals

**Tomorrow's preview:**
You'll write proper table-driven tests for all these token types. Testing before implementing the lexer ensures your types are correct.

---

*Progress: Day 2/30 complete* ‚≠ê‚≠ê
*Module: Token Types Definition*

**Real compiler engineering:** You just defined a language's vocabulary. Every compiler starts here!
