# Day 005: Single-Character Token Scanning

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 1 of 4 - Go Setup & Token Types

---

## üéØ Today's Goal

Implement scanning for single-character tokens - the simplest form of lexical analysis. Your lexer will recognize operators like `(`, `)`, `{`, `}`, `+`, `-`, `*`, `;` and produce corresponding tokens.

**What You'll Build:** Working lexer that scans single-character tokens and handles whitespace.

---

## üìö What You'll Learn Today

**Go Fundamentals:**
- Switch statements for pattern matching
- String slicing (`source[start:current]`)
- Import statements with `fmt`

**Lexing Concepts:**
- Token recognition with switch
- Whitespace handling
- Error reporting for unexpected characters
- Extracting lexemes from source

---

## ‚úÖ Today's Tasks

### Task 1: Add Token Helper Methods (25 minutes)

**What to do:**
Create methods to add tokens to the scanner's token list.

**Add to `pkg/lexer/scanner.go` (after the advance method):**

```go
import (
	"fmt"

	"github.com/bhargav/konfiguru/pkg/tokens"
)

// addToken adds a token with no literal value
func (s *Scanner) addToken(tokenType tokens.TokenType) {
	s.addTokenLiteral(tokenType, nil)
}

// addTokenLiteral adds a token with a literal value
func (s *Scanner) addTokenLiteral(tokenType tokens.TokenType, literal interface{}) {
	text := s.source[s.start:s.current]
	s.tokens = append(s.tokens, tokens.NewToken(
		tokenType,
		text,
		literal,
		s.line,
	))
}

// addError records a scanning error
func (s *Scanner) addError(message string) {
	s.errors = append(s.errors,
		fmt.Sprintf("[line %d] Error: %s", s.line, message))
}
```

**Understanding string slicing:**
```go
source = "var x = 5"
start = 0, current = 3
text := source[0:3]  // "var"
```

---

### Task 2: Implement Single-Character Scanning (40 minutes)

**What to do:**
Replace the scanToken() placeholder with real token recognition logic.

**Replace scanToken() in `pkg/lexer/scanner.go`:**

```go
// scanToken scans a single token
func (s *Scanner) scanToken() {
	c := s.advance()

	switch c {
	case '(':
		s.addToken(tokens.LEFT_PAREN)
	case ')':
		s.addToken(tokens.RIGHT_PAREN)
	case '{':
		s.addToken(tokens.LEFT_BRACE)
	case '}':
		s.addToken(tokens.RIGHT_BRACE)
	case ',':
		s.addToken(tokens.COMMA)
	case '.':
		s.addToken(tokens.DOT)
	case '-':
		s.addToken(tokens.MINUS)
	case '+':
		s.addToken(tokens.PLUS)
	case ';':
		s.addToken(tokens.SEMICOLON)
	case '*':
		s.addToken(tokens.STAR)
	case ' ', '\r', '\t':
		// Ignore whitespace
	case '\n':
		s.line++
	default:
		s.addError("Unexpected character.")
	}
}
```

**How it works:**
1. Get next character with `advance()`
2. Match it against known single-char tokens
3. Add corresponding token to list
4. Ignore whitespace (space, tab, carriage return)
5. Track newlines for line numbers
6. Report errors for unknown characters

---

### Task 3: Write Single-Character Tests (35 minutes)

**What to do:**
Test that single-character tokens are scanned correctly.

**Add to `pkg/lexer/scanner_test.go`:**

```go
func TestScanTokens_SingleCharacters(t *testing.T) {
	source := "(){},.-+;*"
	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	if len(errs) != 0 {
		t.Errorf("got errors: %v", errs)
	}

	expected := []tokens.TokenType{
		tokens.LEFT_PAREN,
		tokens.RIGHT_PAREN,
		tokens.LEFT_BRACE,
		tokens.RIGHT_BRACE,
		tokens.COMMA,
		tokens.DOT,
		tokens.MINUS,
		tokens.PLUS,
		tokens.SEMICOLON,
		tokens.STAR,
		tokens.EOF,
	}

	if len(toks) != len(expected) {
		t.Fatalf("got %d tokens, want %d", len(toks), len(expected))
	}

	for i, expectedType := range expected {
		if toks[i].Type != expectedType {
			t.Errorf("token[%d] type = %v, want %v", i, toks[i].Type, expectedType)
		}
	}
}

func TestScanTokens_Whitespace(t *testing.T) {
	source := "  (  )  \n  {  }  "
	scanner := NewScanner(source)
	toks, errs := scanner.ScanTokens()

	if len(errs) != 0 {
		t.Errorf("got errors: %v", errs)
	}

	// Should have 4 tokens + EOF (whitespace ignored)
	expected := []tokens.TokenType{
		tokens.LEFT_PAREN,
		tokens.RIGHT_PAREN,
		tokens.LEFT_BRACE,
		tokens.RIGHT_BRACE,
		tokens.EOF,
	}

	if len(toks) != len(expected) {
		t.Fatalf("got %d tokens, want %d", len(toks), len(expected))
	}
}

func TestScanTokens_LineTracking(t *testing.T) {
	source := "(\n)\n{\n}"
	scanner := NewScanner(source)
	toks, _ := scanner.ScanTokens()

	expectedLines := []int{1, 2, 3, 4, 4} // EOF on last line
	for i, tok := range toks {
		if tok.Line != expectedLines[i] {
			t.Errorf("token[%d] line = %d, want %d", i, tok.Line, expectedLines[i])
		}
	}
}

func TestScanTokens_UnexpectedCharacter(t *testing.T) {
	source := "(@)"
	scanner := NewScanner(source)
	_, errs := scanner.ScanTokens()

	if len(errs) != 1 {
		t.Fatalf("got %d errors, want 1", len(errs))
	}

	if !strings.Contains(errs[0], "Unexpected character") {
		t.Errorf("error = %q, want to contain 'Unexpected character'", errs[0])
	}
}
```

**Add import for strings:**
```go
import (
	"strings"
	"testing"

	"github.com/bhargav/konfiguru/pkg/tokens"
)
```

---

### Task 4: Run Tests and Verify (25 minutes)

**Run lexer tests:**
```bash
go test ./pkg/lexer -v
```

**Expected output:**
```
=== RUN   TestNewScanner
--- PASS: TestNewScanner (0.00s)
=== RUN   TestScanTokens_Empty
--- PASS: TestScanTokens_Empty (0.00s)
=== RUN   TestIsAtEnd
--- PASS: TestIsAtEnd (0.00s)
=== RUN   TestAdvance
--- PASS: TestAdvance (0.00s)
=== RUN   TestScanTokens_SingleCharacters
--- PASS: TestScanTokens_SingleCharacters (0.00s)
=== RUN   TestScanTokens_Whitespace
--- PASS: TestScanTokens_Whitespace (0.00s)
=== RUN   TestScanTokens_LineTracking
--- PASS: TestScanTokens_LineTracking (0.00s)
=== RUN   TestScanTokens_UnexpectedCharacter
--- PASS: TestScanTokens_UnexpectedCharacter (0.00s)
PASS
```

**Run all tests:**
```bash
go test ./... -v
```

**Check coverage:**
```bash
go test ./pkg/lexer -cover
```

---

### Task 5: Manual Testing with Example Code (20 minutes)

**What to do:**
Test manually by creating a small Go program.

**Create file: `cmd/lox/main.go` (temporary):**

```go
package main

import (
	"fmt"

	"github.com/bhargav/konfiguru/pkg/lexer"
)

func main() {
	source := "(+ - * ;)"
	scanner := lexer.NewScanner(source)
	tokens, errors := scanner.ScanTokens()

	if len(errors) > 0 {
		for _, err := range errors {
			fmt.Println(err)
		}
	}

	for _, token := range tokens {
		fmt.Println(token)
	}
}
```

**Run it:**
```bash
go run ./cmd/lox
```

**Expected output:**
```
LEFT_PAREN ( <nil>
PLUS + <nil>
MINUS - <nil>
STAR * <nil>
SEMICOLON ; <nil>
RIGHT_PAREN ) <nil>
EOF  <nil>
```

---

### Task 6: Commit Single-Character Scanning (15 minutes)

```bash
git add pkg/lexer/
git commit -m "feat: implement single-character token scanning

- Add addToken() and addTokenLiteral() helpers
- Implement scanToken() with switch statement
- Scan single-character tokens: (){},.+-;*
- Handle whitespace (space, tab, \r)
- Track line numbers with \n
- Add error reporting for unexpected characters
- Include comprehensive tests:
  - Single-character tokens
  - Whitespace handling
  - Line tracking
  - Error detection

Lexer can now scan:
- All single-character operators
- Properly ignore whitespace
- Track line numbers for errors
- Report unknown characters

Next: Two-character operators (!=, ==, <=, >=)

Ref: Crafting Interpreters Chapter 4.5 (Recognizing Lexemes)"
```

---

## üìñ Resources

**Essential:**
- [Crafting Interpreters - 4.5 Recognizing Lexemes](https://craftinginterpreters.com/scanning.html#recognizing-lexemes)
- [Go by Example: Switch](https://gobyexample.com/switch)
- [Go Strings and String Slicing](https://go.dev/blog/strings)

---

## ‚úÖ End-of-Day Checklist

- [ ] addToken() helper implemented
- [ ] addTokenLiteral() helper implemented
- [ ] addError() helper implemented
- [ ] scanToken() implemented with switch statement
- [ ] All 10 single-character tokens recognized
- [ ] Whitespace handling working
- [ ] Line tracking working (\n increments line)
- [ ] Error reporting for unexpected characters
- [ ] 4 new test functions added
- [ ] All tests passing
- [ ] Manual testing completed
- [ ] Committed to git

**Time Spent:** ~2.5 hours (25+40+35+25+20+15 = 160 minutes)

---

## üîó Navigation

- [‚Üê Day 004: Scanner Core Structure](Day-004.md)
- [‚Üí Day 006: Two-Character Operators](Day-006.md)
- [‚Üë Back to Month 1](README.md)

---

## üìù Learning Notes

**What did we accomplish?**
Your lexer can now scan real tokens! When given "(+ - *)", it produces LEFT_PAREN, PLUS, MINUS, STAR, RIGHT_PAREN tokens. This is the core of lexical analysis.

**Switch statements in Go:**
Unlike C/Java, Go's switch doesn't fall through by default. Each case is independent. This makes our token recognition clean and safe.

**String slicing:**
`source[start:current]` extracts the substring from start (inclusive) to current (exclusive). This gives us the lexeme.

**Tomorrow's preview:**
You'll handle two-character operators like `!=`, `==`, `<=`, `>=` using lookahead.

---

*Progress: Day 5/30 complete* ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*Module: Single-Character Scanning*

**First working lexer!** It scans real tokens from source code!
