# Day 006: Two-Character Operators & Comments

**Month 1:** Lox Lexer & Go Fundamentals
**Phase:** Foundation
**Week:** 1 of 4 - Go Setup & Token Types

---

## üéØ Today's Goal

Implement lookahead to handle two-character operators (`!=`, `==`, `<=`, `>=`) and comment support (`//`). This requires peeking at the next character without consuming it - a key lexing technique.

**What You'll Build:** Lexer that handles two-character operators and skips comments.

---

## üìö What You'll Learn Today

**Lexing Concepts:**
- Lookahead with `peek()` and `match()`
- Maximal munch principle (longest match wins)
- Comment handling

**Go Patterns:**
- Conditional logic in scanning
- Multiple return points in functions

---

## ‚úÖ Today's Tasks

### Task 1: Add Lookahead Helper Methods (20 minutes)

**Add to `pkg/lexer/scanner.go`:**

```go
// match checks if the current character matches expected
// If it does, consumes it and returns true
func (s *Scanner) match(expected byte) bool {
	if s.isAtEnd() {
		return false
	}
	if s.source[s.current] != expected {
		return false
	}
	s.current++
	return true
}

// peek returns the current character without consuming it
func (s *Scanner) peek() byte {
	if s.isAtEnd() {
		return 0
	}
	return s.source[s.current]
}
```

**How match() works:**
```
Source: "!="
Current: 1 (at '=')

if s.match('='):  // true, current becomes 2
    return BANG_EQUAL
else:
    return BANG
```

---

### Task 2: Implement Two-Character Operators (30 minutes)

**Modify scanToken() in `pkg/lexer/scanner.go`** - add these cases BEFORE the default:

```go
	case '!':
		if s.match('=') {
			s.addToken(tokens.BANG_EQUAL)
		} else {
			s.addToken(tokens.BANG)
		}
	case '=':
		if s.match('=') {
			s.addToken(tokens.EQUAL_EQUAL)
		} else {
			s.addToken(tokens.EQUAL)
		}
	case '<':
		if s.match('=') {
			s.addToken(tokens.LESS_EQUAL)
		} else {
			s.addToken(tokens.LESS)
		}
	case '>':
		if s.match('=') {
			s.addToken(tokens.GREATER_EQUAL)
		} else {
			s.addToken(tokens.GREATER)
		}
```

---

### Task 3: Implement Comment Handling (25 minutes)

**Add to scanToken() BEFORE the default case:**

```go
	case '/':
		if s.match('/') {
			// A comment goes until the end of the line
			for s.peek() != '\n' && !s.isAtEnd() {
				s.advance()
			}
		} else {
			s.addToken(tokens.SLASH)
		}
```

**How it works:**
- `//` starts a comment
- Skip all characters until newline or end of file
- Don't consume the newline (next scanToken() call will handle it)

---

### Task 4: Write Tests for Two-Character Operators (35 minutes)

**Add to `pkg/lexer/scanner_test.go`:**

```go
func TestScanTokens_TwoCharacterOperators(t *testing.T) {
	tests := []struct {
		source   string
		expected []tokens.TokenType
	}{
		{"!=", []tokens.TokenType{tokens.BANG_EQUAL, tokens.EOF}},
		{"!", []tokens.TokenType{tokens.BANG, tokens.EOF}},
		{"==", []tokens.TokenType{tokens.EQUAL_EQUAL, tokens.EOF}},
		{"=", []tokens.TokenType{tokens.EQUAL, tokens.EOF}},
		{"<=", []tokens.TokenType{tokens.LESS_EQUAL, tokens.EOF}},
		{"<", []tokens.TokenType{tokens.LESS, tokens.EOF}},
		{">=", []tokens.TokenType{tokens.GREATER_EQUAL, tokens.EOF}},
		{">", []tokens.TokenType{tokens.GREATER, tokens.EOF}},
		{"! = == !=", []tokens.TokenType{
			tokens.BANG, tokens.EQUAL, tokens.EQUAL_EQUAL, tokens.BANG_EQUAL, tokens.EOF,
		}},
	}

	for _, tt := range tests {
		t.Run(tt.source, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			toks, errs := scanner.ScanTokens()

			if len(errs) != 0 {
				t.Errorf("got errors %v", errs)
			}

			if len(toks) != len(tt.expected) {
				t.Fatalf("got %d tokens, want %d", len(toks), len(tt.expected))
			}

			for i, expectedType := range tt.expected {
				if toks[i].Type != expectedType {
					t.Errorf("token[%d] = %v, want %v", i, toks[i].Type, expectedType)
				}
			}
		})
	}
}

func TestScanTokens_Comments(t *testing.T) {
	tests := []struct {
		name     string
		source   string
		expected []tokens.TokenType
	}{
		{
			name:     "simple comment",
			source:   "// this is a comment",
			expected: []tokens.TokenType{tokens.EOF},
		},
		{
			name:     "comment after token",
			source:   "( // comment\n)",
			expected: []tokens.TokenType{tokens.LEFT_PAREN, tokens.RIGHT_PAREN, tokens.EOF},
		},
		{
			name:     "division vs comment",
			source:   "/",
			expected: []tokens.TokenType{tokens.SLASH, tokens.EOF},
		},
		{
			name:     "two divisions",
			source:   "/ /",
			expected: []tokens.TokenType{tokens.SLASH, tokens.SLASH, tokens.EOF},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			scanner := NewScanner(tt.source)
			toks, errs := scanner.ScanTokens()

			if len(errs) != 0 {
				t.Errorf("got errors %v", errs)
			}

			if len(toks) != len(tt.expected) {
				t.Fatalf("got %d tokens, want %d", len(toks), len(tt.expected))
			}

			for i, expectedType := range tt.expected {
				if toks[i].Type != expectedType {
					t.Errorf("token[%d] = %v, want %v", i, toks[i].Type, expectedType)
				}
			}
		})
	}
}
```

---

### Task 5: Run Tests and Verify (20 minutes)

```bash
go test ./pkg/lexer -v
go test ./... -cover
```

All tests should pass, including the new ones for two-character operators and comments.

---

### Task 6: Commit Two-Character Support (15 minutes)

```bash
git add pkg/lexer/
git commit -m "feat: add two-character operators and comment support

- Implement match() helper for lookahead
- Implement peek() for peeking without consuming
- Handle operators: != == <= >=
- Handle both single and double forms: ! vs !=
- Add // comment support (skip until newline)
- Distinguish between / (division) and // (comment)

Tests added:
- All two-character operator combinations
- Comment scenarios
- Edge cases (single vs double chars)

Lexer now handles:
- Maximal munch (longest match wins)
- Lookahead without consuming
- Comment stripping

Ref: Crafting Interpreters Chapter 4.6 (Longer Lexemes)"
```

---

## ‚úÖ End-of-Day Checklist

- [ ] match() helper implemented
- [ ] peek() helper implemented
- [ ] Two-character operators: != == <= >=
- [ ] Comment handling with //
- [ ] Tests for all operator combinations
- [ ] Tests for comments
- [ ] All tests passing
- [ ] Committed to git

**Time Spent:** ~2.4 hours (20+30+25+35+20+15 = 145 minutes)

---

## üîó Navigation

- [‚Üê Day 005: Single-Character Token Scanning](Day-005.md)
- [‚Üí Day 007: Week 1 Review & Summary](Day-007.md)
- [‚Üë Back to Month 1](README.md)

---

## üìù Learning Notes

**Maximal munch principle:**
When you see `!=`, scan both characters as BANG_EQUAL, not BANG then EQUAL. Always match the longest possible token.

**Lookahead patterns:**
- `peek()`: Look without consuming
- `match()`: Conditionally consume if it matches

Tomorrow you'll review Week 1's progress and prepare for Week 2 (strings, numbers, identifiers).

---

*Progress: Day 6/30 complete* ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*Module: Two-Character Operators*
